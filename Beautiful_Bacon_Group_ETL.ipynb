{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements:\n",
    "US Bureau of Labor Statistic API Key in config.py\n",
    "Postgres credentials in config.py\n",
    "\n",
    "Sample Code Source:\n",
    "https://www.bls.gov/developers/api_python.htm#python2\n",
    "\n",
    "Data website:\n",
    "https://www.kroger.com/p/smithfield-thick-cut-naturally-hickory-smoked-bacon/0007080004125\n",
    "https://data.bls.gov/timeseries/APU0000704111\n",
    "'''\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, func, inspect\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from config import bls_api_key, username, passphrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kroger Smithfield Thick Cut Naturally Hickory Smoked Bacon 16oz pricing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.50\n"
     ]
    }
   ],
   "source": [
    "# Kroger Pricing ETL\n",
    "\n",
    "# Splinter browser\n",
    "executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "\n",
    "# Read url\n",
    "url = 'https://www.kroger.com/p/smithfield-thick-cut-naturally-hickory-smoked-bacon/0007080004125'\n",
    "browser.visit(url)\n",
    "\n",
    "# Browse and look for confirm and click thru\n",
    "browser.find_by_text('Confirm').first.click()\n",
    "\n",
    "# Scrape website and look for price of bacon\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "quotes = soup.find_all('mark',  class_=\"kds-Price-promotional\")\n",
    "\n",
    "# Once price is find, split and just collect the price\n",
    "price=str(quotes[0]).split('>')[1].split('<')[0]\n",
    "price=price.replace(\"$\",\"\")\n",
    "print(price)\n",
    "\n",
    "# Find the date\n",
    "my_date=dt.date.today()\n",
    "\n",
    "# Store the price and date into a dictionary\n",
    "data_dict={\"date\":[my_date],\n",
    "          \"price\":[price]}\n",
    "\n",
    "# Print out data_dict\n",
    "data_dict\n",
    "\n",
    "# Convert into a DataFrame\n",
    "data_pd=pd.DataFrame(data_dict)\n",
    "\n",
    "# Show DataFrame\n",
    "data_pd\n",
    "\n",
    "# Connect to Postgres\n",
    "rds_connection_string = f\"{username}:{passphrase}@localhost:5432/bacon_db\"\n",
    "engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "\n",
    "# Create table to PG Admin\n",
    "data_pd.to_sql(name='kroger_price', con=engine, if_exists='append', index=False)\n",
    "\n",
    "# Close the brower\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Bureau of Labor Statistics Bacon CPI Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REQUEST_SUCCEEDED\n",
      "A new table bacon_cpi has been created in the database bacon_db.\n"
     ]
    }
   ],
   "source": [
    "now_year = dt.date.today().year\n",
    "\n",
    "def download_bls_api(startyear =  now_year - 1, endyear = now_year):\n",
    "    \n",
    "    '''Default to since last year'''\n",
    "\n",
    "    # Define variables\n",
    "    series_id = 'APU0000704111'\n",
    "    \n",
    "    # Convert years to string\n",
    "    startyear = str(startyear)\n",
    "    endyear   = str(endyear)\n",
    "\n",
    "    # Pulling API Data from US Bureau of Labor Statistics\n",
    "    headers = {'Content-type': 'application/json'}\n",
    "    data    = json.dumps({\"seriesid\": [series_id], \"startyear\"  : startyear, \"endyear\" : endyear, \"registrationkey\": bls_api_key})\n",
    "    p       = requests.post('https://api.bls.gov/publicAPI/v2/timeseries/data/', data = data, headers = headers)\n",
    "    json_data = json.loads(p.text)\n",
    "    \n",
    "    # Print Status\n",
    "    print(json_data[\"status\"])\n",
    "    \n",
    "    # Print API return message if it's not empty\n",
    "    if json_data['message']:\n",
    "        print(json_data['message'])\n",
    "        \n",
    "    return json_data\n",
    "\n",
    "def transform_bls_data(json_data):\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    data_dict = {\n",
    "        'year_month'  : [],\n",
    "        'value' : [],\n",
    "    }\n",
    "\n",
    "    for item in json_data['Results']['series'][0]['data']:\n",
    "\n",
    "        year_month  = item['year'] + item['period'].replace('M', '')\n",
    "        value  = float(item['value'])\n",
    "        \n",
    "        data_dict['year_month'].append(year_month)\n",
    "        data_dict['value'].append(value)\n",
    "\n",
    "    data_pd = pd.DataFrame(data_dict)\n",
    "\n",
    "    return data_pd\n",
    "\n",
    "def store_cpi_data_to_db(data_pd):\n",
    "\n",
    "    # Connect to Postgres\n",
    "    rds_connection_string = f\"{username}:{passphrase}@localhost:5432/bacon_db\"\n",
    "    engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "\n",
    "    # Pull the last 24 data points from the database\n",
    "    try:\n",
    "        last_24m = pd.read_sql_query('SELECT * FROM bacon_cpi LIMIT 24', con=engine)\n",
    "    except:\n",
    "        # If the table bacon_cpi doesn't exsit in the database. Create one.\n",
    "        data_pd.to_sql(name='bacon_cpi', con = engine, if_exists='append', index = False)\n",
    "        \n",
    "        print('A new table bacon_cpi has been created in the database bacon_db.')\n",
    "        return\n",
    "    \n",
    "    # Store only the new data to the database\n",
    "    for year_month in data_pd.year_month:\n",
    "        if year_month not in last_24m.year_month.values:\n",
    "        \n",
    "            new_data = data_pd.loc[data_pd.year_month == year_month].copy()\n",
    "        \n",
    "            # Load Bacon CPI Data to Postgres\n",
    "            new_data.to_sql(name='bacon_cpi', con = engine, if_exists='append', index = False)\n",
    "        \n",
    "            print(year_month, 'cpi data has been loaded to the database bacon_db.')\n",
    "            \n",
    "    return\n",
    "\n",
    "# Extract Data\n",
    "json_data = download_bls_api()\n",
    "\n",
    "# Transform data\n",
    "data_pd = transform_bls_data(json_data)\n",
    "\n",
    "# Load data\n",
    "store_cpi_data_to_db(data_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lean Hogs futures from Investing.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lean_hog_data_pull():\n",
    "    # Creating the browser to scrape with\n",
    "    executable_path = {'executable_path': 'chromedriver.exe'}\n",
    "    browser = Browser('chrome', **executable_path, headless=False)\n",
    "    \n",
    "    # Giving the web path to the site we are scraping\n",
    "    url = 'https://www.investing.com/commodities/lean-hogs-historical-data'\n",
    "    browser.visit(url)\n",
    "    \n",
    "    # Iterate through all pages\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Finding the correct table, there are multiple tables on the page, ours is the only with \"Open\" in the table\n",
    "    texts = soup.find_all(\"table\")\n",
    "    for item in texts:\n",
    "        if \"Open\" in item.get_text():\n",
    "            tabletext = item.get_text()\n",
    "            \n",
    "    # Cleaning up some of the text to prepare for a list of lists\n",
    "    tabletext = tabletext.replace(\"\\n\", \"|\")\n",
    "    tabletext = tabletext.replace(\"|-\", \"|0.00K\")\n",
    "    tabletext = tabletext.replace(\"Vol.\", \"Volume|\")\n",
    "    tabletext = tabletext.replace(\"K\", \"K|\")\n",
    "    tabletext = tabletext.replace(\"%\", \"PCT\")\n",
    "    tabletext = tabletext.replace(\" Change\", \"Change\")\n",
    "    \n",
    "    # Splitting the data to create the first list; cleaning it up a bit\n",
    "    tablelist = tabletext.split(\"|||\")\n",
    "    cleanerlist = []\n",
    "    for item in tablelist:\n",
    "        if item != \"\":\n",
    "            cleanerlist.append(item.replace(\"||\",\"\"))\n",
    "            \n",
    "    # Creating a list of lists\n",
    "    listolist = []\n",
    "    for item in tablelist:\n",
    "        listolist.append(item.split(\"|\"))\n",
    "    \n",
    "    #Removing blank lists\n",
    "    listolist = [x for x in listolist if x != ['']]\n",
    "    \n",
    "    #Creating a clean list of lists where each list has the blanks ('') removed\n",
    "    clistolist = []\n",
    "    for item in listolist:\n",
    "        clistolist.append(list(filter(None,item)))\n",
    "    for item in clistolist:\n",
    "        for itemtwo in item:\n",
    "            itemtwo = itemtwo.strip()\n",
    "            \n",
    "    #Turning the clean list of lists into a dataframe\n",
    "    df = pd.DataFrame(clistolist[1:], columns = clistolist[0])\n",
    "    \n",
    "    #Cleaning up the dataframe\n",
    "    df['Volume'] = df['Volume'].str.replace('K','')\n",
    "    df['Date'] = df['Date'].str.replace(\", \",\"-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Jan \",\"01-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Feb \",\"02-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Mar \",\"03-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Apr \",\"04-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"May \",\"05-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Jun \",\"06-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Jul \",\"07-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Aug \",\"08-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Sep \",\"09-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Oct \",\"10-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Nov \",\"11-\")\n",
    "    df['Date'] = df['Date'].str.replace(\"Dec \",\"12-\")\n",
    "    df['Change PCT'] = df['Change PCT'].str.replace(\"PCT\",\"\")\n",
    "    df['Change PCT'] = pd.to_numeric(df['Change PCT']) / 100\n",
    "    df['Volume'] = pd.to_numeric(df['Volume']) * 1000\n",
    "    df['Price'] = pd.to_numeric(df['Price'])\n",
    "    df['Open'] = pd.to_numeric(df['Open'])\n",
    "    df['High'] = pd.to_numeric(df['High'])\n",
    "    df['Low'] = pd.to_numeric(df['Low'])\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    \n",
    "    # Connect to Postgres\n",
    "    rds_connection_string = f\"{username}:{passphrase}@localhost:5432/bacon_db\"\n",
    "    engine = create_engine(f'postgresql://{rds_connection_string}')\n",
    "    \n",
    "    # Pulling the historical data if it exists\n",
    "    try:\n",
    "        olddf = pd.read_sql_table(\"lean_hog_commodity\", con=engine)\n",
    "        fulldf = olddf.append(df,ignore_index=True)\n",
    "    except:\n",
    "        fulldf = df\n",
    "    fulldf = fulldf.drop_duplicates('Date',keep='last')\n",
    "    \n",
    "    # Load Lean Hog Commodity to Postgres\n",
    "    fulldf.to_sql(name='lean_hog_commodity', con = engine, if_exists='replace', index = False)\n",
    "    \n",
    "    # Closing out the browser\n",
    "    browser.quit()\n",
    "    return fulldf\n",
    "\n",
    "fulldf = lean_hog_data_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "via2",
   "language": "python",
   "name": "via2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
